В этом репозитории собраны домашние задания по программированию за пятый семестр. Внизу приведено описание наиболее примечательных заданий в формате [название ветки, имя файла, условие задачи].

## HW3 -- morph.py

В этом задании вам необходимо реализовать свой униграммный морфологический анализатор. В классе UnigramMorphAnalyzer должны быть реализованы следующие методы:

 a) train: считывает слова из размеченного корпуса и накапливает частеречную статистику в соответствии с  окончаниями слов (4, 3, 2, 1 последних символа)

 b) predict: выводит список вероятностей различных частей речи для данного токена 
 
 c) save: сохраняет модель с помощью библиотеки pickle

 d) load: загружает модель
 
 e) При вызове  UnigramMorphAnalyzer[‘ending’] должна выводиться частеречная статистика по указанному окончанию
 
 f) eval: должна выводить точность анализатора на тестовых данных open corpora
 
 
## HW5 -- task1.py
Создать генератор пар ADJF + NOUN в именительном падеже и согласованных по роду с помощью itertools и pymorphy, используя словарь russian_shuffled.txt


## HW7 -- knife_corpus.py; knife_news_ner_analysis.ipynb
В течение следующей недели каждому из вас необходимо будет собрать собственный новостной корпус.

Для этого вам нужно:

1) Найти источник для корпуса: журнал, газету, интернет-издание
2) Спарсить новости (как минимум, за последний календарный год) с помощью requests, bs4 и re. Хранить их нужно будет в CSV или JSON формате (на ваше усмотрение). Для каждой новости нужно хранить её дату, рубрику, ссылку и собственно саму новость. Можно добавить ещё и другую мета информацию. За неё добавим баллов сверху :)

После того как соберете свой корпус, проведите извлечение именованных сущностей с помощью Natasha. Для каждой рубрики посмотрите на распределение частот и представителей этих сущностей.
Готовый анализ представьте в виде jupyter notebook. Скрипт на парсинг сайта лучше оформить в виде .py (по PEP8, с использованием аргументов и кэширования при необходимости)

Корпус на GitHub грузить не надо. Залейте его на google/yandex/etc drive и прикрепите ссылку в readme ветки


## HW8 -- pds.ipynb
1) Найдите самые короткие твиты авторов, author_name которых состоит из, как минимум, двух слов. Выведите через пробел: author_name и твит
2) Выведите частотный список всех эмодзи, использованных в твитах
3) Используя функцию apply, посчитайте сумму sentiment score всех твитов, используя словари: https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/tree/master/data/opinion-lexicon-English
    1 для позитивных слов
    -1 для негативных

Используйте WordNetLemmatizer
Результат запишите в новую колонку sentiment_score
